{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8865a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XAS Spectra PCA + Clustering Pipeline\n",
    "======================================\n",
    "Reads pre-normalized XANES/EXAFS spectra, performs PCA to identify\n",
    "the number of distinct spectral components, then clusters spectra\n",
    "in PC-score space to group similar grains.\n",
    "\n",
    "Assumes spectra are already normalized/flattened (e.g., via Athena or Larch).\n",
    "Uses the flattened (post-edge-corrected) XANES by default to avoid\n",
    "post-edge slope artifacts in PCA.\n",
    "\n",
    "Requirements:\n",
    "    pip install xraylarch scikit-learn scipy matplotlib numpy pandas\n",
    "\n",
    "Usage:\n",
    "    1. Update the CONFIGURATION section below with your paths and parameters.\n",
    "    2. Run: python xas_pca_clustering.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Larch imports\n",
    "from larch import Group\n",
    "from larch.math import pca_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e065ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Directory containing your normalized spectra files\n",
    "SPECTRA_DIR = Path(\"./flattened-spectra\")\n",
    "\n",
    "# Energy range to use for PCA (eV, relative to E0 or absolute)\n",
    "# Set to None to use the full overlapping range\n",
    "E_MIN = 7100  # e.g., -20 (relative to E0) or 7100 (absolute)\n",
    "E_MAX = 7180  # e.g., 80 (relative to E0) or 7200 (absolute)\n",
    "ENERGY_IS_RELATIVE = False  # True if E_MIN/E_MAX are relative to E0\n",
    "\n",
    "# PCA region: 'xanes' uses normalized mu(E), 'exafs' uses chi(k)\n",
    "PCA_REGION = \"xanes\"\n",
    "\n",
    "# Common energy/k grid spacing for interpolation\n",
    "E_STEP = 0.2  # eV step for XANES\n",
    "\n",
    "# Max number of clusters to evaluate\n",
    "MAX_CLUSTERS = 15\n",
    "\n",
    "# Set to None to use silhouette-optimal k, or an integer to override\n",
    "FORCE_K = 5\n",
    "\n",
    "# Number of PCA components to use for clustering and target transformation.\n",
    "# Set to None to use the IND minimum (automatic), or override with an integer\n",
    "# if IND gives an unreasonable result (common with noisy microprobe data).\n",
    "N_COMPONENTS = 5\n",
    "\n",
    "# Reference spectra for target transformation (optional)\n",
    "# List of file paths to reference spectra\n",
    "REFERENCE_DIR = Path(\"./FeK-standards/fluorescence/flattened\")\n",
    "REFERENCE_FILES = [\"2L-Fhy on sand.csv\",\n",
    "                    \"2L-Fhy.csv\",\n",
    "                    \"6L-Fhy.csv\",\n",
    "                    \"Augite.csv\",\n",
    "                    \"Biotite.csv\",\n",
    "                    \"FeS.csv\",\n",
    "                    \"Ferrosmectite.csv\",\n",
    "                    \"Goethite on sand.csv\",\n",
    "                    \"Goethite.csv\",\n",
    "                    \"Green Rust - Carbonate.csv\",\n",
    "                    \"Green Rust - Chloride.csv\",\n",
    "                    \"Green Rust - Sulfate.csv\",\n",
    "                    \"Hematite on sand.csv\",\n",
    "                    \"Hematite.csv\",\n",
    "                    \"Hornblende.csv\",\n",
    "                    \"Ilmenite.csv\",\n",
    "                    \"Jarosite.csv\",\n",
    "                    \"Lepidocrocite.csv\",\n",
    "                    \"Mackinawite (aged).csv\",\n",
    "                    \"Mackinawite.csv\",\n",
    "                    \"Maghemite.csv\",\n",
    "                    \"Nontronite.csv\",\n",
    "                    \"Pyrite.csv\",\n",
    "                    \"Pyrrhotite.csv\",\n",
    "                    \"Schwertmannite.csv\",\n",
    "                    \"Siderite-n.csv\",\n",
    "                    \"Siderite-s.csv\",\n",
    "                    \"Vivianite.csv\"] \n",
    "                    \n",
    "REFERENCE_PATHS = [REFERENCE_DIR / f for f in REFERENCE_FILES]\n",
    "# Output directory\n",
    "\n",
    "# Bulk directory\n",
    "BULK_DIR = Path(\"./bulk\")\n",
    "BULK_PATTERN = '*.csv'\n",
    "\n",
    "OUTPUT_DIR = Path(\"./pca_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bace834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ascii_spectra(spectra_dir, pattern=\"*.csv\"):\n",
    "    \"\"\"\n",
    "    Load pre-normalized/flattened spectra from individual CSV files.\n",
    "    Expected format: comment lines starting with #, then two columns\n",
    "    (energy, flat) comma-separated.\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    files = sorted(Path(spectra_dir).glob(pattern))\n",
    "    for f in files:\n",
    "        try:\n",
    "            data = np.loadtxt(str(f), delimiter=\",\", comments=\"#\")\n",
    "            g = Group(\n",
    "                energy=data[:, 0],\n",
    "                flat=data[:, 1],\n",
    "                filename=f.stem,\n",
    "                _name=f.stem,\n",
    "            )\n",
    "            groups.append(g)\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipping {f.name}: {e}\")\n",
    "    print(f\"Loaded {len(groups)} spectra from {spectra_dir}\")\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d647cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xanes_matrix(groups, e_min=None, e_max=None):\n",
    "    \"\"\"\n",
    "    Interpolate normalized XANES spectra onto a common energy grid.\n",
    "    Returns: energy_grid (1D), matrix (n_spectra x n_energy), names list\n",
    "    \"\"\"\n",
    "    # Find the common energy range\n",
    "    all_emin = max(g.energy.min() for g in groups)\n",
    "    all_emax = min(g.energy.max() for g in groups)\n",
    "\n",
    "    if e_min is not None:\n",
    "        if ENERGY_IS_RELATIVE:\n",
    "            # Use median E0 as reference\n",
    "            e0_median = np.median([g.e0 for g in groups])\n",
    "            all_emin = max(all_emin, e0_median + e_min)\n",
    "        else:\n",
    "            all_emin = max(all_emin, e_min)\n",
    "\n",
    "    if e_max is not None:\n",
    "        if ENERGY_IS_RELATIVE:\n",
    "            e0_median = np.median([g.e0 for g in groups])\n",
    "            all_emax = min(all_emax, e0_median + e_max)\n",
    "        else:\n",
    "            all_emax = min(all_emax, e_max)\n",
    "\n",
    "    energy_grid = np.arange(all_emin, all_emax, E_STEP)\n",
    "    matrix = np.zeros((len(groups), len(energy_grid)))\n",
    "    names = []\n",
    "\n",
    "    for i, g in enumerate(groups):\n",
    "        matrix[i, :] = np.interp(energy_grid, g.energy, g.flat)\n",
    "        names.append(g._name)\n",
    "\n",
    "    print(f\"Spectral matrix: {matrix.shape[0]} spectra × {matrix.shape[1]} energy points\")\n",
    "    print(f\"Energy range: {energy_grid[0]:.1f} – {energy_grid[-1]:.1f} eV\")\n",
    "    return energy_grid, matrix, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77162ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUALITY SCREENING\n",
    "# ============================================================\n",
    "\n",
    "def screen_spectra(matrix, names, sigma_threshold=3.0):\n",
    "    \"\"\"\n",
    "    Remove outlier spectra based on their distance from the mean spectrum.\n",
    "    Returns filtered matrix and names.\n",
    "    \"\"\"\n",
    "    mean_spec = matrix.mean(axis=0)\n",
    "    distances = np.sqrt(np.sum((matrix - mean_spec) ** 2, axis=1))\n",
    "    threshold = distances.mean() + sigma_threshold * distances.std()\n",
    "\n",
    "    mask = distances < threshold\n",
    "    n_removed = (~mask).sum()\n",
    "    if n_removed > 0:\n",
    "        print(f\"Quality screen: removed {n_removed} spectra beyond {sigma_threshold}σ\")\n",
    "        removed_names = [names[i] for i in range(len(names)) if not mask[i]]\n",
    "        for rn in removed_names:\n",
    "            print(f\"    Removed: {rn}\")\n",
    "    else:\n",
    "        print(\"Quality screen: all spectra passed\")\n",
    "\n",
    "    filtered_names = [names[i] for i in range(len(names)) if mask[i]]\n",
    "    return matrix[mask], filtered_names, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "919aef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PCA\n",
    "# ============================================================\n",
    "\n",
    "def run_pca(x_grid, matrix, names):\n",
    "    \"\"\"\n",
    "    Run PCA using Larch's pca_train.\n",
    "    Returns the PCA result group.\n",
    "    \"\"\"\n",
    "    # Build Larch groups for pca_train\n",
    "    groups_for_pca = []\n",
    "    for i in range(matrix.shape[0]):\n",
    "        g = Group()\n",
    "        if PCA_REGION == \"xanes\":\n",
    "            g.energy = x_grid\n",
    "            g.flat = matrix[i, :]\n",
    "        else:\n",
    "            g.k = x_grid\n",
    "            g.chi = matrix[i, :] / (x_grid ** K_WEIGHT)  # undo k-weight\n",
    "        g._name = names[i]\n",
    "        groups_for_pca.append(g)\n",
    "\n",
    "    if PCA_REGION == \"xanes\":\n",
    "        pca_result = pca_train(groups_for_pca, arrayname=\"flat\")\n",
    "    else:\n",
    "        pca_result = pca_train(groups_for_pca, arrayname=\"chi\")\n",
    "\n",
    "    return pca_result\n",
    "\n",
    "\n",
    "def plot_pca_diagnostics(pca_result, output_dir):\n",
    "    \"\"\"Plot scree plot, IND, and component spectra.\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    n_show = min(20, len(pca_result.variances))\n",
    "\n",
    "    # --- Scree plot ---\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.semilogy(range(1, n_show + 1), pca_result.variances[:n_show], \"ko-\")\n",
    "    ax1.set_xlabel(\"Component number\")\n",
    "    ax1.set_ylabel(\"Eigenvalue (variance)\")\n",
    "    ax1.set_title(\"Scree Plot\")\n",
    "    ax1.set_xticks(range(1, n_show + 1))\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- IND function ---\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ind = pca_result.ind\n",
    "    ax2.semilogy(range(1, len(ind) + 1), ind, \"rs-\")\n",
    "    ind_min = np.argmin(ind) + 1\n",
    "    ax2.axvline(ind_min, color=\"blue\", linestyle=\"--\", label=f\"IND min = {ind_min}\")\n",
    "    ax2.set_xlabel(\"Component number\")\n",
    "    ax2.set_ylabel(\"IND\")\n",
    "    ax2.set_title(f\"Indicator Function (minimum at {ind_min} components)\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- First few component spectra ---\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    n_comp_show = min(5, ind_min + 2)\n",
    "    for i in range(n_comp_show):\n",
    "        offset = i * 0.5\n",
    "        ax3.plot(pca_result.components[i] + offset, label=f\"PC{i+1}\")\n",
    "    ax3.set_xlabel(\"Point index\")\n",
    "    ax3.set_ylabel(\"Component loading (offset)\")\n",
    "    ax3.set_title(\"Principal Component Spectra\")\n",
    "    ax3.legend(fontsize=8)\n",
    "\n",
    "    # --- Cumulative variance ---\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    cumvar = np.cumsum(pca_result.variances) / np.sum(pca_result.variances) * 100\n",
    "    ax4.plot(range(1, n_show + 1), cumvar[:n_show], \"go-\")\n",
    "    ax4.axhline(95, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"95%\")\n",
    "    ax4.axhline(99, color=\"red\", linestyle=\":\", alpha=0.5, label=\"99%\")\n",
    "    ax4.set_xlabel(\"Number of components\")\n",
    "    ax4.set_ylabel(\"Cumulative variance (%)\")\n",
    "    ax4.set_title(\"Cumulative Variance Explained\")\n",
    "    ax4.legend()\n",
    "    ax4.set_xticks(range(1, n_show + 1))\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.savefig(output_dir / \"pca_diagnostics.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: pca_diagnostics.png\")\n",
    "    return ind_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2293dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLUSTERING\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_kmeans(scores, max_k):\n",
    "    \"\"\"Evaluate k-means for different numbers of clusters.\"\"\"\n",
    "    results = {\"k\": [], \"inertia\": [], \"silhouette\": []}\n",
    "    for k in range(2, max_k + 1):\n",
    "        km = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "        labels = km.fit_predict(scores)\n",
    "        results[\"k\"].append(k)\n",
    "        results[\"inertia\"].append(km.inertia_)\n",
    "        results[\"silhouette\"].append(silhouette_score(scores, labels))\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_clustering(scores, names, n_components, output_dir):\n",
    "    \"\"\"\n",
    "    Run k-means and hierarchical clustering on PCA scores.\n",
    "    Produces diagnostic plots and returns cluster assignments.\n",
    "    \"\"\"\n",
    "    max_k = min(MAX_CLUSTERS, len(names) - 1)\n",
    "\n",
    "    # --- Evaluate k-means ---\n",
    "    km_results = evaluate_kmeans(scores, max_k)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Elbow plot\n",
    "    axes[0].plot(km_results[\"k\"], km_results[\"inertia\"], \"ko-\")\n",
    "    axes[0].set_xlabel(\"Number of clusters (k)\")\n",
    "    axes[0].set_ylabel(\"Inertia (within-cluster SS)\")\n",
    "    axes[0].set_title(\"K-Means Elbow Plot\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Silhouette plot\n",
    "    axes[1].plot(km_results[\"k\"], km_results[\"silhouette\"], \"bs-\")\n",
    "    best_k = km_results[\"k\"][np.argmax(km_results[\"silhouette\"])]\n",
    "    axes[1].axvline(best_k, color=\"red\", linestyle=\"--\",\n",
    "                    label=f\"Best k = {best_k}\")\n",
    "    axes[1].set_xlabel(\"Number of clusters (k)\")\n",
    "    axes[1].set_ylabel(\"Silhouette score\")\n",
    "    axes[1].set_title(\"Silhouette Analysis\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"kmeans_evaluation.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: kmeans_evaluation.png\")\n",
    "    print(f\"Optimal k by silhouette: {best_k}\")\n",
    "\n",
    "    # --- Hierarchical clustering dendrogram ---\n",
    "    linkage_matrix = linkage(pdist(scores, metric=\"euclidean\"), method=\"ward\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    dendrogram(\n",
    "        linkage_matrix,\n",
    "        labels=names,\n",
    "        leaf_rotation=90,\n",
    "        leaf_font_size=6,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\"Hierarchical Clustering Dendrogram (Ward linkage)\")\n",
    "    ax.set_ylabel(\"Distance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"dendrogram.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: dendrogram.png\")\n",
    "\n",
    "    # --- Score scatter plots with cluster assignments ---\n",
    "    km_best = KMeans(n_clusters=best_k, n_init=20, random_state=42)\n",
    "    labels = km_best.fit_predict(scores)\n",
    "\n",
    "    n_pairs = min(3, n_components - 1)\n",
    "    fig, axes = plt.subplots(1, n_pairs, figsize=(6 * n_pairs, 5))\n",
    "    if n_pairs == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    pc_pairs = [(0, 1), (0, 2), (1, 2)][:n_pairs]\n",
    "    for ax, (i, j) in zip(axes, pc_pairs):\n",
    "        scatter = ax.scatter(scores[:, i], scores[:, j], c=labels,\n",
    "                             cmap=\"tab10\", s=40, edgecolors=\"k\", linewidths=0.5)\n",
    "        ax.set_xlabel(f\"PC{i+1} score\")\n",
    "        ax.set_ylabel(f\"PC{j+1} score\")\n",
    "        ax.set_title(f\"PC{i+1} vs PC{j+1}\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[-1], label=\"Cluster\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"score_scatter.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: score_scatter.png\")\n",
    "\n",
    "    return labels, best_k, km_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c77686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLUSTER INSPECTION\n",
    "# ============================================================\n",
    "\n",
    "def plot_cluster_spectra(x_grid, matrix, names, labels, n_clusters, output_dir):\n",
    "    \"\"\"Plot all spectra in each cluster, plus cluster centroids.\"\"\"\n",
    "    xlabel = \"Energy (eV)\" if PCA_REGION == \"xanes\" else f\"k (Å⁻¹)\"\n",
    "\n",
    "    # Individual cluster plots\n",
    "    fig, axes = plt.subplots(1, n_clusters, figsize=(5 * n_clusters, 5),\n",
    "                             sharey=True)\n",
    "    if n_clusters == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    centroids = []\n",
    "    for ci in range(n_clusters):\n",
    "        mask = labels == ci\n",
    "        cluster_spectra = matrix[mask]\n",
    "        centroid = cluster_spectra.mean(axis=0)\n",
    "        centroids.append(centroid)\n",
    "        cluster_names = [names[i] for i in range(len(names)) if mask[i]]\n",
    "\n",
    "        ax = axes[ci]\n",
    "        for spec in cluster_spectra:\n",
    "            ax.plot(x_grid, spec, alpha=0.3, color=f\"C{ci}\")\n",
    "        ax.plot(x_grid, centroid, \"k-\", linewidth=2, label=\"Centroid\")\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_title(f\"Cluster {ci+1} (n={mask.sum()})\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Flattened μ(E)\" if PCA_REGION == \"xanes\" else f\"k^{K_WEIGHT}·χ(k)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"cluster_spectra.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: cluster_spectra.png\")\n",
    "\n",
    "    # Overlay of all centroids\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    for ci, centroid in enumerate(centroids):\n",
    "        ax.plot(x_grid, centroid, label=f\"Cluster {ci+1}\", linewidth=2)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(\"Flattened μ(E)\" if PCA_REGION == \"xanes\" else f\"k^{K_WEIGHT}·χ(k)\")\n",
    "    ax.set_title(\"Cluster Centroids Comparison\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"cluster_centroids.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: cluster_centroids.png\")\n",
    "\n",
    "    return np.array(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ca8384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OUTLIER / RARE PHASE DETECTION\n",
    "# ============================================================\n",
    "\n",
    "def find_outliers(scores, names, labels, n_sigma=2.5):\n",
    "    \"\"\"\n",
    "    Identify spectra that are far from their cluster centroid.\n",
    "    These may represent rare phases or mixed grains.\n",
    "    \"\"\"\n",
    "    outliers = []\n",
    "    for ci in np.unique(labels):\n",
    "        mask = labels == ci\n",
    "        cluster_scores = scores[mask]\n",
    "        centroid = cluster_scores.mean(axis=0)\n",
    "        distances = np.sqrt(np.sum((cluster_scores - centroid) ** 2, axis=1))\n",
    "        threshold = distances.mean() + n_sigma * distances.std()\n",
    "\n",
    "        cluster_names = [names[i] for i in range(len(names)) if mask[i]]\n",
    "        for idx, (d, name) in enumerate(zip(distances, cluster_names)):\n",
    "            if d > threshold:\n",
    "                outliers.append({\n",
    "                    \"name\": name,\n",
    "                    \"cluster\": ci,\n",
    "                    \"distance\": d,\n",
    "                    \"threshold\": threshold,\n",
    "                })\n",
    "\n",
    "    if outliers:\n",
    "        print(f\"\\nPotential outliers / rare phases ({len(outliers)} spectra):\")\n",
    "        for o in outliers:\n",
    "            print(f\"  {o['name']} (cluster {o['cluster']+1}, \"\n",
    "                  f\"dist={o['distance']:.3f}, threshold={o['threshold']:.3f})\")\n",
    "    else:\n",
    "        print(\"\\nNo outliers detected at the current threshold.\")\n",
    "\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b40dbda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TARGET TRANSFORMATION (optional)\n",
    "# ============================================================\n",
    "\n",
    "def run_target_transform(pca_result, reference_files, n_components):\n",
    "    \"\"\"\n",
    "    Test reference spectra against PCA model by manual projection.\n",
    "    Interpolates each reference onto the PCA energy grid, projects onto\n",
    "    the first n components, reconstructs, and computes residual.\n",
    "    \"\"\"\n",
    "    if not reference_files:\n",
    "        print(\"\\nNo reference files provided — skipping target transformation.\")\n",
    "        return None\n",
    "\n",
    "    energy_grid = pca_result.x\n",
    "    components = pca_result.components[:n_components]\n",
    "    mean_spec = pca_result.mean\n",
    "\n",
    "    print(f\"\\nTarget transformation results (using {n_components} components):\")\n",
    "    print(f\"{'Reference':<30s} {'Chi-square':>12s} {'R-factor':>10s}\")\n",
    "    print(\"-\" * 54)\n",
    "\n",
    "    results = []\n",
    "    for ref_path in reference_files:\n",
    "        try:\n",
    "            ref_dat = np.loadtxt(str(ref_path), delimiter=\",\", comments=\"#\")\n",
    "            ref_energy = ref_dat[:, 0]\n",
    "            ref_flat = ref_dat[:, 1]\n",
    "\n",
    "            # Interpolate reference onto PCA energy grid\n",
    "            ref_interp = np.interp(energy_grid, ref_energy, ref_flat)\n",
    "\n",
    "            # Mean-center and project onto components\n",
    "            centered = ref_interp - mean_spec\n",
    "            weights = centered @ components.T\n",
    "            reconstructed = weights @ components + mean_spec\n",
    "\n",
    "            # Compute fit quality\n",
    "            residual = ref_interp - reconstructed\n",
    "            chi_sq = np.sum(residual ** 2) / len(residual)\n",
    "            r_factor = np.sum(np.abs(residual)) / np.sum(np.abs(ref_interp))\n",
    "\n",
    "            print(f\"{Path(ref_path).stem:<30s} {chi_sq:>12.6f} {r_factor:>10.4f}\")\n",
    "            results.append({\n",
    "                \"reference\": Path(ref_path).stem,\n",
    "                \"chi_square\": chi_sq,\n",
    "                \"r_factor\": r_factor,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"{Path(ref_path).stem:<30s} FAILED: {e}\")\n",
    "\n",
    "    # Sort by chi-square\n",
    "    if results:\n",
    "        results.sort(key=lambda x: x[\"chi_square\"])\n",
    "        print(f\"\\nRanked by fit quality (best first):\")\n",
    "        for i, r in enumerate(results, 1):\n",
    "            print(f\"  {i:2d}. {r['reference']:<30s} χ²={r['chi_square']:.6f}  R={r['r_factor']:.4f}\")\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"  Lower chi-square / R-factor = better reconstruction from PCA components.\")\n",
    "    print(\"  Well-reconstructed references are consistent with species in the dataset.\")\n",
    "    print(\"  Poor reconstruction suggests that species is not represented.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbdfdbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPORT RESULTS\n",
    "# ============================================================\n",
    "\n",
    "def export_results(names, labels, scores, n_components, outliers, output_dir):\n",
    "    \"\"\"Save cluster assignments and scores to CSV.\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"spectrum\": names,\n",
    "        \"cluster\": labels + 1,  # 1-indexed for readability\n",
    "    })\n",
    "    for i in range(n_components):\n",
    "        df[f\"PC{i+1}_score\"] = scores[:, i]\n",
    "\n",
    "    df[\"is_outlier\"] = df[\"spectrum\"].isin([o[\"name\"] for o in outliers])\n",
    "\n",
    "    outpath = output_dir / \"cluster_assignments.csv\"\n",
    "    df.to_csv(outpath, index=False)\n",
    "    print(f\"\\nSaved: {outpath}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c379396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS FOR INTERACTIVE EXPLORATION\n",
    "# ============================================================\n",
    "\n",
    "def override_clusters(scores, k):\n",
    "    \"\"\"Re-cluster with a specific k.\"\"\"\n",
    "    km = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "    labels = km.fit_predict(scores)\n",
    "    print(f\"Clustered into k={k}:\")\n",
    "    for i in range(k):\n",
    "        print(f\"  Cluster {i+1}: {(labels==i).sum()} spectra\")\n",
    "    return labels, k\n",
    "\n",
    "def plot_score_scatter(scores, labels, n_components, best_k, output_dir=None, suffix=\"\"):\n",
    "    \"\"\"Score scatter plots colored by cluster labels.\"\"\"\n",
    "    n_pairs = min(3, n_components - 1)\n",
    "    fig, axes = plt.subplots(1, n_pairs, figsize=(6 * n_pairs, 5))\n",
    "    if n_pairs == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    pc_pairs = [(0, 1), (0, 2), (1, 2)][:n_pairs]\n",
    "    for ax, (i, j) in zip(axes, pc_pairs):\n",
    "        scatter = ax.scatter(scores[:, i], scores[:, j], c=labels,\n",
    "                             cmap=\"tab10\", s=40, edgecolors=\"k\", linewidths=0.5)\n",
    "        ax.set_xlabel(f\"PC{i+1} score\")\n",
    "        ax.set_ylabel(f\"PC{j+1} score\")\n",
    "        ax.set_title(f\"PC{i+1} vs PC{j+1}\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.colorbar(scatter, ax=axes[-1], label=\"Cluster\", ticks=range(best_k))\n",
    "    plt.tight_layout()\n",
    "    if output_dir:\n",
    "        plt.savefig(output_dir / f\"score_scatter{suffix}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0939db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 172 spectra from flattened-spectra\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Step 1: Load data\n",
    "groups = load_ascii_spectra(SPECTRA_DIR)\n",
    "if len(groups) < 3:\n",
    "    raise ValueError(f\"Need at least 3 spectra for PCA, got {len(groups)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a3b9b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral matrix: 172 spectra × 400 energy points\n",
      "Energy range: 7100.0 – 7179.8 eV\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build spectral matrix\n",
    "x_grid, matrix, names = build_xanes_matrix(groups, E_MIN, E_MAX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0230350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality screen: all spectra passed\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Quality screening\n",
    "matrix, names, quality_mask = screen_spectra(matrix, names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: PCA\n",
    "pca_result = run_pca(x_grid, matrix, names)\n",
    "ind_min = plot_pca_diagnostics(pca_result, OUTPUT_DIR)\n",
    "\n",
    "# Determine number of components\n",
    "if N_COMPONENTS is not None:\n",
    "    n_components = N_COMPONENTS\n",
    "    print(f\"Using manual override: {n_components} components\")\n",
    "elif ind_min > len(names) // 2:\n",
    "    cumvar = np.cumsum(pca_result.variances) / np.sum(pca_result.variances)\n",
    "    n_components = int(np.argmax(cumvar >= 0.95)) + 1\n",
    "    print(f\"IND minimum unreliable ({ind_min}). Using 95% variance cutoff: {n_components} components\")\n",
    "else:\n",
    "    n_components = ind_min\n",
    "    print(f\"Number of significant components (IND): {n_components}\")\n",
    "\n",
    "scores = (pca_result.ydat - pca_result.mean) @ pca_result.components[:n_components].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Target transformation\n",
    "tt_results = run_target_transform(pca_result, REFERENCE_PATHS, n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6a: Clustering evaluation (silhouette + dendrogram)\n",
    "labels, best_k, km_model = run_clustering(scores, names, n_components, OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6b: Override cluster count and visualize\n",
    "# Change k here and re-run this cell + cells below to explore different groupings\n",
    "labels, best_k = override_clusters(scores, k=FORCE_K or best_k)\n",
    "plot_score_scatter(scores, labels, n_components, best_k, OUTPUT_DIR, suffix=f\"_k{best_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Cluster inspection\n",
    "centroids = plot_cluster_spectra(x_grid, matrix, names, labels, best_k, OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0678fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCF of cluster centroids against reference library\n",
    "from itertools import combinations\n",
    "\n",
    "def lcf_centroids(x_grid, centroids, reference_paths, max_refs=4, e_min=None, e_max=None):\n",
    "    \"\"\"\n",
    "    Linear combination fitting of each cluster centroid against reference spectra.\n",
    "    Tests all combinations of 1 to max_refs references and picks the best fit.\n",
    "    \"\"\"\n",
    "    # Load and interpolate references\n",
    "    refs = {}\n",
    "    for ref_path in reference_paths:\n",
    "        try:\n",
    "            ref_dat = np.loadtxt(str(ref_path), delimiter=\",\", comments=\"#\")\n",
    "            ref_interp = np.interp(x_grid, ref_dat[:, 0], ref_dat[:, 1])\n",
    "            refs[Path(ref_path).stem] = ref_interp\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipping {Path(ref_path).stem}: {e}\")\n",
    "\n",
    "    ref_names = list(refs.keys())\n",
    "    ref_matrix = np.array([refs[n] for n in ref_names])\n",
    "\n",
    "    # Optional energy mask\n",
    "    if e_min is not None or e_max is not None:\n",
    "        mask = np.ones(len(x_grid), dtype=bool)\n",
    "        if e_min is not None:\n",
    "            mask &= x_grid >= e_min\n",
    "        if e_max is not None:\n",
    "            mask &= x_grid <= e_max\n",
    "    else:\n",
    "        mask = np.ones(len(x_grid), dtype=bool)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for ci, centroid in enumerate(centroids):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Cluster {ci+1} centroid\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        best_fit = {\"r_factor\": np.inf}\n",
    "\n",
    "        # Try combinations of 1 to max_refs references\n",
    "        for n_ref in range(1, max_refs + 1):\n",
    "            for combo in combinations(range(len(ref_names)), n_ref):\n",
    "                combo_names = [ref_names[i] for i in combo]\n",
    "                A = ref_matrix[list(combo)][:, mask].T\n",
    "                b = centroid[mask]\n",
    "\n",
    "                # Non-negative least squares\n",
    "                from scipy.optimize import nnls\n",
    "                weights, rnorm = nnls(A, b)\n",
    "\n",
    "                # Compute fit quality\n",
    "                fitted = A @ weights\n",
    "                residual = b - fitted\n",
    "                r_factor = np.sum(np.abs(residual)) / np.sum(np.abs(b))\n",
    "                chi_sq = np.sum(residual ** 2) / len(residual)\n",
    "                weight_sum = np.sum(weights)\n",
    "\n",
    "                if r_factor < best_fit[\"r_factor\"]:\n",
    "                    best_fit = {\n",
    "                        \"refs\": combo_names,\n",
    "                        \"weights\": weights,\n",
    "                        \"weight_sum\": weight_sum,\n",
    "                        \"r_factor\": r_factor,\n",
    "                        \"chi_sq\": chi_sq,\n",
    "                        \"fitted\": np.zeros_like(centroid),\n",
    "                        \"n_refs\": n_ref,\n",
    "                    }\n",
    "                    best_fit[\"fitted\"][mask] = fitted\n",
    "\n",
    "        # Print best fit\n",
    "        print(f\"Best fit ({best_fit['n_refs']} components, R={best_fit['r_factor']:.4f}, \"\n",
    "              f\"χ²={best_fit['chi_sq']:.6f}):\")\n",
    "        for name, w in zip(best_fit[\"refs\"], best_fit[\"weights\"]):\n",
    "            pct = w / best_fit[\"weight_sum\"] * 100 if best_fit[\"weight_sum\"] > 0 else 0\n",
    "            print(f\"  {name:<30s} {w:.4f}  ({pct:.1f}%)\")\n",
    "        print(f\"  {'Sum':<30s} {best_fit['weight_sum']:.4f}\")\n",
    "\n",
    "        best_fit[\"cluster\"] = ci + 1\n",
    "        all_results.append(best_fit)\n",
    "\n",
    "    # Plot fits\n",
    "    n_clusters = len(centroids)\n",
    "    fig, axes = plt.subplots(1, n_clusters, figsize=(5 * n_clusters, 5), sharey=True)\n",
    "    if n_clusters == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ci, (centroid, result) in enumerate(zip(centroids, all_results)):\n",
    "        ax = axes[ci]\n",
    "        ax.plot(x_grid, centroid, \"k-\", linewidth=2, label=\"Centroid\")\n",
    "        ax.plot(x_grid, result[\"fitted\"], \"r--\", linewidth=1.5, label=f\"LCF (R={result['r_factor']:.4f})\")\n",
    "        ax.plot(x_grid, centroid - result[\"fitted\"], \"g-\", alpha=0.5, label=\"Residual\")\n",
    "\n",
    "        # Build legend with components\n",
    "        legend_text = \"\\n\".join(\n",
    "            f\"{n}: {w/result['weight_sum']*100:.0f}%\"\n",
    "            for n, w in zip(result[\"refs\"], result[\"weights\"])\n",
    "        )\n",
    "        ax.text(0.98, 0.98, legend_text, transform=ax.transAxes,\n",
    "                fontsize=7, verticalalignment=\"top\", horizontalalignment=\"right\",\n",
    "                bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8))\n",
    "\n",
    "        ax.set_xlabel(\"Energy (eV)\")\n",
    "        ax.set_title(f\"Cluster {ci+1}\")\n",
    "        ax.legend(fontsize=7, loc=\"lower right\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Flattened μ(E)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"lcf_centroids.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return all_results\n",
    "\n",
    "lcf_results = lcf_centroids(x_grid, centroids, REFERENCE_PATHS, max_refs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9062a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCF of individual spectra against reference library\n",
    "    \n",
    "def lcf_individual_spectra(x_grid, matrix, names, labels, reference_paths, \n",
    "                           max_refs=3, e_min=None, e_max=None):\n",
    "    \"\"\"\n",
    "    Linear combination fitting of each individual spectrum against reference spectra.\n",
    "    Returns a DataFrame with fit results for every spectrum.\n",
    "    \"\"\"\n",
    "    from scipy.optimize import nnls\n",
    "    from itertools import combinations\n",
    "\n",
    "    # Load and interpolate references\n",
    "    refs = {}\n",
    "    for ref_path in reference_paths:\n",
    "        try:\n",
    "            ref_dat = np.loadtxt(str(ref_path), delimiter=\",\", comments=\"#\")\n",
    "            ref_interp = np.interp(x_grid, ref_dat[:, 0], ref_dat[:, 1])\n",
    "            refs[Path(ref_path).stem] = ref_interp\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipping {Path(ref_path).stem}: {e}\")\n",
    "\n",
    "    ref_names = list(refs.keys())\n",
    "    ref_matrix = np.array([refs[n] for n in ref_names])\n",
    "\n",
    "    # Energy mask\n",
    "    mask = np.ones(len(x_grid), dtype=bool)\n",
    "    if e_min is not None:\n",
    "        mask &= x_grid >= e_min\n",
    "    if e_max is not None:\n",
    "        mask &= x_grid <= e_max\n",
    "\n",
    "    # Precompute all reference combinations\n",
    "    all_combos = []\n",
    "    for n_ref in range(1, max_refs + 1):\n",
    "        for combo in combinations(range(len(ref_names)), n_ref):\n",
    "            all_combos.append(combo)\n",
    "    print(f\"Testing {len(all_combos)} reference combinations per spectrum \"\n",
    "          f\"({len(matrix)} spectra = {len(all_combos) * len(matrix):,} total fits)\")\n",
    "\n",
    "    # Fit each spectrum\n",
    "    results = []\n",
    "    for si in range(len(matrix)):\n",
    "        spectrum = matrix[si]\n",
    "        b = spectrum[mask]\n",
    "        best = {\"r_factor\": np.inf}\n",
    "\n",
    "        for combo in all_combos:\n",
    "            A = ref_matrix[list(combo)][:, mask].T\n",
    "            weights, _ = nnls(A, b)\n",
    "            fitted = A @ weights\n",
    "            residual = b - fitted\n",
    "            r_factor = np.sum(np.abs(residual)) / np.sum(np.abs(b))\n",
    "\n",
    "            if r_factor < best[\"r_factor\"]:\n",
    "                best = {\n",
    "                    \"refs\": [ref_names[i] for i in combo],\n",
    "                    \"weights\": weights,\n",
    "                    \"r_factor\": r_factor,\n",
    "                    \"chi_sq\": np.sum(residual ** 2) / len(residual),\n",
    "                    \"weight_sum\": np.sum(weights),\n",
    "                }\n",
    "\n",
    "        # Build result row\n",
    "        row = {\n",
    "            \"spectrum\": names[si],\n",
    "            \"cluster\": labels[si] + 1,\n",
    "            \"r_factor\": best[\"r_factor\"],\n",
    "            \"chi_sq\": best[\"chi_sq\"],\n",
    "            \"weight_sum\": best[\"weight_sum\"],\n",
    "            \"n_refs\": len(best[\"refs\"]),\n",
    "        }\n",
    "        for rn, w in zip(best[\"refs\"], best[\"weights\"]):\n",
    "            row[rn] = w / best[\"weight_sum\"] if best[\"weight_sum\"] > 0 else 0\n",
    "        results.append(row)\n",
    "\n",
    "        if (si + 1) % 25 == 0 or si == len(matrix) - 1:\n",
    "            print(f\"  Fitted {si+1}/{len(matrix)} spectra\")\n",
    "\n",
    "    df = pd.DataFrame(results).fillna(0)\n",
    "\n",
    "    # Sort reference columns by frequency of appearance\n",
    "    ref_cols = [c for c in df.columns if c in ref_names]\n",
    "    ref_cols_sorted = sorted(ref_cols, key=lambda c: (df[c] > 0).sum(), reverse=True)\n",
    "    meta_cols = [\"spectrum\", \"cluster\", \"r_factor\", \"chi_sq\", \"weight_sum\", \"n_refs\"]\n",
    "    df = df[meta_cols + ref_cols_sorted]\n",
    "\n",
    "    # Print summary by cluster\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY: Average composition by cluster (normalized %)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for ci in sorted(df[\"cluster\"].unique()):\n",
    "        cdf = df[df[\"cluster\"] == ci]\n",
    "        print(f\"\\nCluster {ci} (n={len(cdf)}, mean R={cdf['r_factor'].mean():.4f}):\")\n",
    "        for col in ref_cols_sorted:\n",
    "            mean_pct = cdf[col].mean() * 100\n",
    "            if mean_pct > 1:\n",
    "                print(f\"  {col:<30s} {mean_pct:5.1f}%\")\n",
    "\n",
    "    # Print poorly fit spectra\n",
    "    poor_fits = df[df[\"r_factor\"] > 0.05].sort_values(\"r_factor\", ascending=False)\n",
    "    if len(poor_fits) > 0:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"POORLY FIT SPECTRA (R > 0.05): {len(poor_fits)} spectra\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for _, row in poor_fits.iterrows():\n",
    "            print(f\"  {row['spectrum']:<50s} R={row['r_factor']:.4f}  cluster={row['cluster']}\")\n",
    "    else:\n",
    "        print(f\"\\nAll spectra fit with R < 0.05\")\n",
    "\n",
    "    # Save\n",
    "    outpath = OUTPUT_DIR / \"lcf_individual.csv\"\n",
    "    df.to_csv(outpath, index=False, float_format=\"%.4f\")\n",
    "    print(f\"\\nSaved: {outpath}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "lcf_df = lcf_individual_spectra(x_grid, matrix, names, labels, REFERENCE_PATHS, max_refs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec18ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Outlier / rare phase detection\n",
    "outliers = find_outliers(scores, names, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Export results\n",
    "df = export_results(names, labels, scores, n_components, outliers, OUTPUT_DIR)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nTotal spectra analyzed: {len(names)}\")\n",
    "print(f\"PCA components: {n_components}\")\n",
    "print(f\"Clusters: {best_k}\")\n",
    "for ci in range(best_k):\n",
    "    print(f\"  Cluster {ci+1}: {(labels == ci).sum()} spectra\")\n",
    "print(f\"Outliers: {len(outliers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c506d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCF of bulk spectra using mineral references informed by PCA results\n",
    "#\n",
    "# The PCA/clustering identified these dominant phases:\n",
    "#   - Hornblende, Augite, Biotite (silicates)\n",
    "#   - 6L-Fhy, Ferrosmectite (Fe(III) phases)\n",
    "#   - Mackinawite, Mackinawite (aged), Pyrrhotite (sulfides)\n",
    "#   - Siderite-s, Siderite-n (carbonates)\n",
    "#   - Ilmenite (heavy mineral)\n",
    "#   - Vivianite (phosphate)\n",
    "#\n",
    "# Adjust this list based on your interpretation of the PCA results.\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# References to use for bulk LCF — selected from PCA/clustering results\n",
    "# Group by mineralogical class to avoid redundancy\n",
    "BULK_REFS = {\n",
    "    # name: path — pick one representative per spectral type\n",
    "    \"Hornblende\":         REFERENCE_DIR / \"Hornblende.csv\",\n",
    "    \"Biotite\":            REFERENCE_DIR / \"Biotite.csv\",\n",
    "    \"6L-Fhy\":             REFERENCE_DIR / \"6L-Fhy.csv\",\n",
    "    \"Ferrosmectite\":      REFERENCE_DIR / \"Ferrosmectite.csv\",\n",
    "    \"Goethite\":           REFERENCE_DIR / \"Goethite.csv\",\n",
    "    \"Mackinawite (aged)\": REFERENCE_DIR / \"Mackinawite (aged).csv\",\n",
    "    \"Siderite-s\":         REFERENCE_DIR / \"Siderite-s.csv\",\n",
    "    \"Ilmenite\":           REFERENCE_DIR / \"Ilmenite.csv\",\n",
    "    \"Pyrrhotite\":         REFERENCE_DIR / \"Pyrrhotite.csv\",\n",
    "    \"Vivianite\":          REFERENCE_DIR / \"Vivianite.csv\",\n",
    "}\n",
    "\n",
    "# Max number of references per fit\n",
    "BULK_MAX_REFS = 4\n",
    "\n",
    "BULK_OUTPUT_DIR = BULK_DIR\n",
    "\n",
    "# Phase grouping for plotting\n",
    "PHASE_GROUPS = {\n",
    "    \"Fe(III) oxyhydroxide\":    [\"6L-Fhy\", \"Goethite\"],\n",
    "    \"Fe(III) phyllosilicate\":  [\"Ferrosmectite\"],\n",
    "    \"Fe(II) phyllosilicate\":   [\"Biotite\"],\n",
    "    \"Fe(II) silicate\":         [\"Hornblende\"],\n",
    "    \"Fe sulfide\":              [\"Mackinawite (aged)\", \"Pyrrhotite\"],\n",
    "    \"Fe(II) carbonate\":        [\"Siderite-s\"],\n",
    "    \"Fe-Ti oxide\":             [\"Ilmenite\"],\n",
    "    \"Fe(II) phosphate\":        [\"Vivianite\"],\n",
    "}\n",
    "\n",
    "# Fixed plot order and colors\n",
    "PLOT_ORDER = [\n",
    "    \"Fe(II) silicate\",\n",
    "   # \"Fe(II) phyllosilicate\",\n",
    "    \"Fe(III) oxyhydroxide\",\n",
    "    \"Fe(III) phyllosilicate\",\n",
    "]\n",
    "\n",
    "GROUP_COLORS = {\n",
    "    \"Fe(III) oxyhydroxide\":    \"#d62728\",\n",
    "    \"Fe(III) phyllosilicate\":  \"#7fbfff\",\n",
    "    \"Fe(II) phyllosilicate\":   \"#aec7e8\",\n",
    "    \"Fe(II) silicate\":         \"#1f77b4\",\n",
    "    \"Fe sulfide\":              \"#2ca02c\",\n",
    "    \"Fe(II) carbonate\":        \"#ff7f0e\",\n",
    "    \"Fe-Ti oxide\":             \"#9467bd\",\n",
    "    \"Fe(II) phosphate\":        \"#8c564b\",\n",
    "}\n",
    "# ============================================================\n",
    "# FIT\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_bulk_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract station and depth from filename.\n",
    "    Example: GP17_station5_19119_1cm_015_A_fl.avg -> station='GP17_station5_19119', depth=1.0\n",
    "    \"\"\"\n",
    "    stem = Path(filename).stem\n",
    "    depth_match = re.search(r'(\\d+\\.?\\d*)cm', stem)\n",
    "    if not depth_match:\n",
    "        return None, None\n",
    "    depth = float(depth_match.group(1))\n",
    "    station = stem[:depth_match.start()].rstrip('_')\n",
    "    return station, depth\n",
    "\n",
    "    \n",
    "def lcf_bulk_with_references(x_grid, bulk_dir, bulk_refs, max_refs=4, pattern=\"*.csv\"):\n",
    "    \"\"\"\n",
    "    Fit each bulk spectrum as a linear combination of mineral references.\n",
    "    Tests all combinations of 1 to max_refs and picks the best fit.\n",
    "    \"\"\"\n",
    "    from scipy.optimize import nnls\n",
    "    from itertools import combinations\n",
    "\n",
    "    # Load and interpolate references\n",
    "    ref_names = list(bulk_refs.keys())\n",
    "    ref_spectra = {}\n",
    "    for name, path in bulk_refs.items():\n",
    "        try:\n",
    "            dat = np.loadtxt(str(path), delimiter=\",\", comments=\"#\")\n",
    "            ref_spectra[name] = np.interp(x_grid, dat[:, 0], dat[:, 1])\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipping {name}: {e}\")\n",
    "    \n",
    "    ref_names = list(ref_spectra.keys())\n",
    "    ref_matrix = np.array([ref_spectra[n] for n in ref_names])\n",
    "    \n",
    "    # Precompute combos\n",
    "    all_combos = []\n",
    "    for n in range(1, max_refs + 1):\n",
    "        for combo in combinations(range(len(ref_names)), n):\n",
    "            all_combos.append(combo)\n",
    "    print(f\"Using {len(ref_names)} references, testing {len(all_combos)} combinations per spectrum\")\n",
    "\n",
    "    # Load and fit bulk spectra\n",
    "    bulk_files = sorted(Path(bulk_dir).glob(pattern))\n",
    "    print(f\"Found {len(bulk_files)} bulk spectra\")\n",
    "\n",
    "    results = []\n",
    "    for bf in bulk_files:\n",
    "        station, depth = parse_bulk_filename(bf.name)\n",
    "        if station is None:\n",
    "            print(f\"  Skipping {bf.name}: could not parse depth\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            dat = np.loadtxt(str(bf), delimiter=\",\", comments=\"#\")\n",
    "            bulk_interp = np.interp(x_grid, dat[:, 0], dat[:, 1])\n",
    "\n",
    "            best = {\"r_factor\": np.inf}\n",
    "            for combo in all_combos:\n",
    "                A = ref_matrix[list(combo)].T\n",
    "                weights, _ = nnls(A, bulk_interp)\n",
    "                fitted = A @ weights\n",
    "                residual = bulk_interp - fitted\n",
    "                r_factor = np.sum(np.abs(residual)) / np.sum(np.abs(bulk_interp))\n",
    "\n",
    "                if r_factor < best[\"r_factor\"]:\n",
    "                    best = {\n",
    "                        \"refs\": [ref_names[i] for i in combo],\n",
    "                        \"weights\": weights,\n",
    "                        \"r_factor\": r_factor,\n",
    "                        \"chi_sq\": np.sum(residual ** 2) / len(residual),\n",
    "                        \"weight_sum\": np.sum(weights),\n",
    "                        \"fitted\": fitted,\n",
    "                    }\n",
    "\n",
    "            row = {\n",
    "                \"filename\": bf.name,\n",
    "                \"station\": station,\n",
    "                \"depth_cm\": depth,\n",
    "                \"r_factor\": best[\"r_factor\"],\n",
    "                \"chi_sq\": best[\"chi_sq\"],\n",
    "                \"weight_sum\": best[\"weight_sum\"],\n",
    "                \"n_refs\": len(best[\"refs\"]),\n",
    "                \"components\": \", \".join(best[\"refs\"]),\n",
    "            }\n",
    "            for rn, w in zip(best[\"refs\"], best[\"weights\"]):\n",
    "                row[rn] = w / best[\"weight_sum\"] if best[\"weight_sum\"] > 0 else 0\n",
    "            results.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error fitting {bf.name}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(results).fillna(0).sort_values([\"station\", \"depth_cm\"])\n",
    "    print(f\"Fit {len(df)} spectra across {df['station'].nunique()} stations\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE GROUPING\n",
    "# ============================================================\n",
    "\n",
    "def aggregate_phases(df, phase_groups):\n",
    "    \"\"\"Sum individual mineral fractions into geochemical classes.\"\"\"\n",
    "    meta_cols = [\"filename\", \"station\", \"depth_cm\", \"r_factor\", \"chi_sq\", \"weight_sum\", \"n_refs\", \"components\"]\n",
    "    df_grouped = df[[c for c in meta_cols if c in df.columns]].copy()\n",
    "    for group_name, members in phase_groups.items():\n",
    "        cols = [c for c in members if c in df.columns]\n",
    "        df_grouped[group_name] = df[cols].sum(axis=1)\n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PLOTTING\n",
    "# ============================================================\n",
    "\n",
    "def plot_bulk_depth_profiles(df, bulk_refs, output_dir):\n",
    "    \"\"\"Stacked area depth profiles using mineral reference fractions.\"\"\"\n",
    "    ref_names = list(bulk_refs.keys())\n",
    "    phase_cols = [c for c in ref_names if c in df.columns]\n",
    "    stations = sorted(df[\"station\"].unique())\n",
    "    n_stations = len(stations)\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(phase_cols)))\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_stations, figsize=(5 * n_stations, 8), sharey=True)\n",
    "    if n_stations == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, station in zip(axes, stations):\n",
    "        sdf = df[df[\"station\"] == station].sort_values(\"depth_cm\")\n",
    "        depths = sdf[\"depth_cm\"].values\n",
    "\n",
    "        cumulative = np.zeros(len(depths))\n",
    "        for col, color in zip(phase_cols, colors):\n",
    "            values = sdf[col].values * 100 if col in sdf.columns else np.zeros(len(depths))\n",
    "            ax.fill_betweenx(depths, cumulative, cumulative + values,\n",
    "                             color=color, label=col, alpha=0.85,\n",
    "                             edgecolor=\"white\", linewidth=0.5)\n",
    "            cumulative += values\n",
    "\n",
    "        ax.set_xlabel(\"Phase fraction (%)\")\n",
    "        ax.set_title(station, fontsize=9)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlim(0, 105)\n",
    "        ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "    axes[0].set_ylabel(\"Depth (cm)\")\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    axes[-1].legend(handles, labels, bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"bulk_depth_profiles.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_grouped_depth_profiles(df, phase_groups, group_colors, output_dir, \n",
    "                                plot_order=None, n_top=3):\n",
    "    \"\"\"Stacked area depth profiles using grouped phase fractions.\"\"\"\n",
    "    phase_cols = [g for g in phase_groups.keys() if g in df.columns]\n",
    "    stations = sorted(df[\"station\"].unique())\n",
    "    n_stations = len(stations)\n",
    "\n",
    "    # Use fixed order if provided, otherwise rank by abundance\n",
    "    if plot_order:\n",
    "        top_phases = [p for p in plot_order if p in phase_cols]\n",
    "    else:\n",
    "        mean_abundance = {col: df[col].mean() for col in phase_cols}\n",
    "        top_phases = sorted(mean_abundance, key=mean_abundance.get, reverse=True)[:n_top]\n",
    "    \n",
    "    other_phases = [c for c in phase_cols if c not in top_phases]\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_stations, figsize=(5 * n_stations, 8), sharey=True)\n",
    "    if n_stations == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, station in zip(axes, stations):\n",
    "        sdf = df[df[\"station\"] == station].sort_values(\"depth_cm\")\n",
    "        depths = sdf[\"depth_cm\"].values\n",
    "\n",
    "        cumulative = np.zeros(len(depths))\n",
    "        for col in top_phases:\n",
    "            values = sdf[col].values * 100\n",
    "            color = group_colors.get(col, \"gray\")\n",
    "            ax.fill_betweenx(depths, cumulative, cumulative + values,\n",
    "                             color=color, label=col, alpha=0.85,\n",
    "                             edgecolor=\"white\", linewidth=0.5)\n",
    "            cumulative += values\n",
    "\n",
    "        # Lump remaining phases as \"Other\"\n",
    "        other_values = sdf[other_phases].sum(axis=1).values * 100\n",
    "        if other_values.sum() > 0:\n",
    "            ax.fill_betweenx(depths, cumulative, cumulative + other_values,\n",
    "                             color=\"lightgray\", label=\"Other\", alpha=0.7,\n",
    "                             edgecolor=\"white\", linewidth=0.5)\n",
    "\n",
    "        # Mark actual sample depths\n",
    "        for d in depths:\n",
    "            ax.axhline(d, color=\"black\", linewidth=0.5, linestyle=\"-\", alpha=0.3)\n",
    "        ax.set_yticks(depths)\n",
    "\n",
    "        ax.set_xlabel(\"Phase fraction (%)\")\n",
    "        ax.set_title(station, fontsize=9)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlim(0, 105)\n",
    "        ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "    axes[0].set_ylabel(\"Depth (cm)\")\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    axes[-1].legend(by_label.values(), by_label.keys(),\n",
    "                    bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"bulk_depth_profiles_grouped.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bulk_fits(x_grid, bulk_dir, bulk_refs, df, output_dir, pattern=\"*.csv\"):\n",
    "    \"\"\"Plot individual LCF fits for each bulk spectrum.\"\"\"\n",
    "    ref_spectra = {}\n",
    "    for name, path in bulk_refs.items():\n",
    "        try:\n",
    "            dat = np.loadtxt(str(path), delimiter=\",\", comments=\"#\")\n",
    "            ref_spectra[name] = np.interp(x_grid, dat[:, 0], dat[:, 1])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    stations = sorted(df[\"station\"].unique())\n",
    "    for station in stations:\n",
    "        sdf = df[df[\"station\"] == station].sort_values(\"depth_cm\")\n",
    "        n_spectra = len(sdf)\n",
    "        ncols = min(4, n_spectra)\n",
    "        nrows = int(np.ceil(n_spectra / ncols))\n",
    "\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 4 * nrows), sharex=True, sharey=True)\n",
    "        axes = np.atleast_2d(axes)\n",
    "\n",
    "        for idx, (_, row) in enumerate(sdf.iterrows()):\n",
    "            ax = axes[idx // ncols, idx % ncols]\n",
    "\n",
    "            # Load bulk spectrum\n",
    "            bf = Path(bulk_dir) / row[\"filename\"]\n",
    "            dat = np.loadtxt(str(bf), delimiter=\",\", comments=\"#\")\n",
    "            bulk_interp = np.interp(x_grid, dat[:, 0], dat[:, 1])\n",
    "\n",
    "            # Reconstruct fit\n",
    "            components = row[\"components\"].split(\", \")\n",
    "            fitted = np.zeros_like(x_grid)\n",
    "            for ref_name in components:\n",
    "                if ref_name in ref_spectra and ref_name in row.index:\n",
    "                    w = row[ref_name] * row[\"weight_sum\"]\n",
    "                    fitted += w * ref_spectra[ref_name]\n",
    "\n",
    "            ax.plot(x_grid, bulk_interp, \"k-\", linewidth=1.5, label=\"Data\")\n",
    "            ax.plot(x_grid, fitted, \"r--\", linewidth=1.2, label=f\"Fit (R={row['r_factor']:.4f})\")\n",
    "            ax.plot(x_grid, bulk_interp - fitted, \"g-\", alpha=0.5, linewidth=0.8, label=\"Residual\")\n",
    "\n",
    "            # Component annotation\n",
    "            comp_text = \"\\n\".join(\n",
    "                f\"{c}: {row[c]*100:.0f}%\" for c in components if c in row.index and row[c] > 0\n",
    "            )\n",
    "            ax.text(0.98, 0.98, comp_text, transform=ax.transAxes, fontsize=6,\n",
    "                    va=\"top\", ha=\"right\", bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8))\n",
    "\n",
    "            ax.set_title(f\"{row['depth_cm']:.0f} cm\", fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            if idx == 0:\n",
    "                ax.legend(fontsize=6)\n",
    "\n",
    "        # Hide empty subplots\n",
    "        for idx in range(n_spectra, nrows * ncols):\n",
    "            axes[idx // ncols, idx % ncols].set_visible(False)\n",
    "\n",
    "        fig.suptitle(station, fontsize=12)\n",
    "        fig.supxlabel(\"Energy (eV)\")\n",
    "        fig.supylabel(\"Flattened μ(E)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / f\"bulk_fits_{station}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "# ============================================================\n",
    "\n",
    "bulk_ref_df = lcf_bulk_with_references(x_grid, BULK_DIR, BULK_REFS,\n",
    "                                        max_refs=BULK_MAX_REFS, pattern=BULK_PATTERN)\n",
    "\n",
    "# Save mineral-level results\n",
    "bulk_ref_df.to_csv(BULK_OUTPUT_DIR / \"bulk_lcf_mineral_refs.csv\", index=False, float_format=\"%.4f\")\n",
    "print(f\"\\nSaved: {BULK_OUTPUT_DIR / 'bulk_lcf_mineral_refs.csv'}\")\n",
    "\n",
    "# Print table\n",
    "print(f\"\\n{'='*80}\")\n",
    "ref_cols = [c for c in BULK_REFS.keys() if c in bulk_ref_df.columns]\n",
    "for station in sorted(bulk_ref_df[\"station\"].unique()):\n",
    "    sdf = bulk_ref_df[bulk_ref_df[\"station\"] == station].sort_values(\"depth_cm\")\n",
    "    print(f\"\\n{station}:\")\n",
    "    print(f\"  {'Depth':>6s} {'R':>7s} {'Sum':>5s}  Components\")\n",
    "    print(f\"  \" + \"-\" * 70)\n",
    "    for _, row in sdf.iterrows():\n",
    "        comps = []\n",
    "        for c in ref_cols:\n",
    "            if row.get(c, 0) > 0.01:\n",
    "                comps.append(f\"{c} {row[c]*100:.0f}%\")\n",
    "        print(f\"  {row['depth_cm']:>5.0f}  {row['r_factor']:>.4f} {row['weight_sum']:>5.2f}  {', '.join(comps)}\")\n",
    "\n",
    "# Aggregate into geochemical classes\n",
    "bulk_grouped = aggregate_phases(bulk_ref_df, PHASE_GROUPS)\n",
    "bulk_grouped.to_csv(BULK_OUTPUT_DIR / \"bulk_lcf_grouped.csv\", index=False, float_format=\"%.4f\")\n",
    "print(f\"\\nSaved: {BULK_OUTPUT_DIR / 'bulk_lcf_grouped.csv'}\")\n",
    "\n",
    "# Plots — mineral-level\n",
    "plot_bulk_depth_profiles(bulk_ref_df, BULK_REFS, BULK_OUTPUT_DIR)\n",
    "\n",
    "# Plots — grouped by geochemical class\n",
    "plot_grouped_depth_profiles(bulk_grouped, PHASE_GROUPS, GROUP_COLORS, BULK_OUTPUT_DIR,\n",
    "                            plot_order=PLOT_ORDER)\n",
    "# Individual fit plots\n",
    "plot_bulk_fits(x_grid, BULK_DIR, BULK_REFS, bulk_ref_df, BULK_OUTPUT_DIR, pattern=BULK_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bulk spectra vs cluster 2 centroid for comparison\n",
    "def plot_bulk_vs_centroid(x_grid, centroids, bulk_dir, pattern=\"*.csv\", centroid_idx=1):\n",
    "    \"\"\"Overlay bulk spectra with a specific cluster centroid.\"\"\"\n",
    "    bulk_files = sorted(Path(bulk_dir).glob(pattern))\n",
    "    \n",
    "    stations = {}\n",
    "    for bf in bulk_files:\n",
    "        station, depth = parse_bulk_filename(bf.name)\n",
    "        if station not in stations:\n",
    "            stations[station] = []\n",
    "        try:\n",
    "            dat = np.loadtxt(str(bf), delimiter=\",\", comments=\"#\")\n",
    "            interp = np.interp(x_grid, dat[:, 0], dat[:, 1])\n",
    "            stations[station].append((depth, interp, bf.name))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    n_stations = len(stations)\n",
    "    fig, axes = plt.subplots(1, n_stations, figsize=(6 * n_stations, 5), sharey=True)\n",
    "    if n_stations == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (station, spectra) in zip(axes, sorted(stations.items())):\n",
    "        # Plot centroid\n",
    "        ax.plot(x_grid, centroids[centroid_idx], \"k-\", linewidth=2.5, \n",
    "                label=f\"Centroid {centroid_idx+1}\", zorder=10)\n",
    "        \n",
    "        # Plot bulk spectra colored by depth\n",
    "        spectra.sort(key=lambda x: x[0])\n",
    "        depths = [s[0] for s in spectra]\n",
    "        cmap = plt.cm.viridis(np.linspace(0, 1, len(spectra)))\n",
    "        \n",
    "        for (depth, spec, _), color in zip(spectra, cmap):\n",
    "            ax.plot(x_grid, spec, color=color, alpha=0.7, linewidth=1, label=f\"{depth:.0f} cm\")\n",
    "        \n",
    "        ax.set_xlabel(\"Energy (eV)\")\n",
    "        ax.set_title(station, fontsize=9)\n",
    "        ax.legend(fontsize=6, ncol=2)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0].set_ylabel(\"Flattened μ(E)\")\n",
    "    plt.suptitle(f\"Bulk spectra vs Cluster {centroid_idx+1} centroid\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_bulk_vs_centroid(x_grid, centroids, BULK_DIR, pattern=BULK_PATTERN, centroid_idx=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
