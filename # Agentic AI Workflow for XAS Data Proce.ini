# Agentic AI Workflow for XAS Data Processing with Larch

## Architecture Overview

The agent operates as a **supervisory loop**: it ingests raw XAS scans, applies larch processing functions, evaluates quality metrics at each step, and makes decisions about parameter adjustments — mimicking the iterative judgment a spectroscopist applies manually in Athena/Larch GUI.

```
┌─────────────────────────────────────────────────────┐
│                   USER / CALLER                     │
│  Provides: raw data files, edge energy (or element/ │
│  edge), any special instructions                    │
└──────────────┬──────────────────────────────────────┘
               │
               ▼
┌─────────────────────────────────────────────────────┐
│              AGENT SUPERVISORY LOOP                 │
│                                                     │
│  1. Ingest & Inspect ──► quality gate               │
│  2. Background Subtract & Normalize ──► quality gate│
│  3. Align ──► quality gate                          │
│  4. Merge ──► quality gate                          │
│  5. Export & Report                                 │
└─────────────────────────────────────────────────────┘
```

Each **quality gate** is where the agent applies judgment: evaluating metrics, flagging outliers, adjusting parameters, or requesting human input if confidence is low.

---

## Core Design Principles

1. **Deterministic tools, probabilistic decisions.** Larch functions are deterministic given parameters. The agent's value-add is selecting parameters and evaluating outputs.
2. **Fail loudly.** If a scan looks anomalous or a normalization is poor, the agent should flag it rather than silently proceeding.
3. **Auditable.** Every parameter choice and quality assessment gets logged so the user can review the agent's reasoning.
4. **Human-in-the-loop optional.** The agent can run fully autonomously for routine data, but escalate edge cases.

---

## Tool Definitions (for LLM function-calling)

These are the tools the agent can call. Each wraps larch functionality with structured I/O.

### Tool 1: `read_xas_scan`
```python
def read_xas_scan(filepath: str, energy_col: str = "energy", mu_col: str = "mutrans") -> dict:
    """
    Read a single XAS scan from file.
    
    Returns:
        {
            "id": str,           # scan identifier
            "energy": list,      # energy array (eV)
            "mu": list,          # mu(E) array
            "n_points": int,
            "energy_range": [float, float],
            "metadata": dict     # any header info
        }
    """
```

### Tool 2: `pre_edge_normalize`
```python
def pre_edge_normalize(
    scan_id: str,
    e0: float | None = None,        # if None, auto-detect
    pre1: float = -150,              # pre-edge start (relative to E0)
    pre2: float = -30,               # pre-edge end
    norm1: float = 50,               # normalization start
    norm2: float = None,             # normalization end (default: end of data)
    nnorm: int = 2,                  # polynomial order for post-edge
    nvict: int = 0                   # energy-dependent normalization
) -> dict:
    """
    Background subtract and normalize a scan using larch.pre_edge().
    
    Returns:
        {
            "scan_id": str,
            "e0": float,
            "edge_step": float,
            "pre_edge_params": dict,
            "norm_params": dict,
            "quality_metrics": {
                "edge_step_magnitude": float,
                "pre_edge_flatness": float,    # R² of pre-edge fit residuals
                "post_edge_flatness": float,   # R² of post-edge normalization
                "norm_range_eV": float,        # extent of post-edge used
            },
            "normalized_mu": list,
            "flattened_mu": list
        }
    """
```

### Tool 3: `align_scans`
```python
def align_scans(
    scan_ids: list[str],
    reference_id: str | None = None,   # if None, use first scan
    alignment_method: str = "derivative_max"  # or "half_edge", "fixed_e0"
) -> dict:
    """
    Align multiple scans using larch.xas.align_group().
    
    Returns:
        {
            "reference_id": str,
            "shifts": {scan_id: float},     # energy shifts applied (eV)
            "max_shift": float,
            "quality_metrics": {
                "mean_shift": float,
                "std_shift": float,
                "outlier_scans": list[str]  # shifts > 2σ from mean
            }
        }
    """
```

### Tool 4: `merge_scans`
```python
def merge_scans(
    scan_ids: list[str],
    weights: dict | None = None,    # {scan_id: weight}, default = equal
    exclude_ids: list[str] = []
) -> dict:
    """
    Merge aligned, normalized scans using larch.xas.merge_groups().
    
    Returns:
        {
            "merged_id": str,
            "n_scans_merged": int,
            "excluded_scans": list[str],
            "quality_metrics": {
                "std_mu": list,              # point-wise std of merged scans
                "max_std": float,
                "mean_std": float,
                "scan_deviations": {scan_id: float}  # per-scan mean deviation from merge
            },
            "merged_energy": list,
            "merged_mu": list,
            "merged_std": list
        }
    """
```

### Tool 5: `evaluate_scan_quality`
```python
def evaluate_scan_quality(scan_id: str) -> dict:
    """
    Comprehensive quality assessment of a single normalized scan.
    
    Returns:
        {
            "scan_id": str,
            "edge_step": float,
            "noise_level": float,          # estimated from post-edge oscillations
            "glitch_detected": bool,
            "glitch_energies": list[float],
            "truncation_warning": bool,     # insufficient pre-edge or post-edge range
            "overall_quality": str          # "good", "marginal", "poor"
        }
    """
```

### Tool 6: `export_results`
```python
def export_results(
    scan_ids: list[str],
    output_dir: str,
    formats: list[str] = ["athena_project", "csv", "plot_png"]
) -> dict:
    """
    Export processed data and generate summary plots.
    """
```

---

## Agent System Prompt (Template)

```markdown
You are an X-ray Absorption Spectroscopy data processing agent. You process 
raw XANES scans through background subtraction, normalization, alignment, and 
merging using the larch software package.

## Workflow

For each dataset provided:

### Step 1: Ingest & Inspect
- Read all scan files using `read_xas_scan`
- Check energy ranges are consistent across scans
- Identify the absorption edge (element and edge provided by user, or infer from energy range)
- Flag any scans with anomalous energy ranges or missing data

### Step 2: Background Subtract & Normalize
- Apply `pre_edge_normalize` to each scan
- Start with default parameters appropriate for the edge:
  - K-edges: pre1=-150, pre2=-30, norm1=50, norm2=end
  - L-edges: narrower ranges, typically pre1=-100, pre2=-20, norm1=30
- Evaluate quality metrics for each scan:
  - edge_step should be positive and physically reasonable (typically 0.1–2.0 for transmission)
  - pre_edge_flatness R² > 0.95
  - post_edge should normalize to ~1.0 without strong curvature
- If quality is poor, retry with adjusted parameters:
  - Narrow pre-edge range if it overlaps another edge
  - Adjust norm range if post-edge is short or has EXAFS oscillations interfering
  - Try different nnorm (1 vs 2 vs 3)
- Flag scans that remain poor after adjustment

### Step 3: Align
- Use `align_scans` with derivative maximum method (default)
- Reference scan: choose scan with best quality metrics, or first scan
- Check alignment shifts:
  - Typical beamline drift: < 0.5 eV between scans in a session
  - Shifts > 1.0 eV: flag as suspicious (possible sample change, beam dump, etc.)
  - Shifts > 2.0 eV: exclude from merge unless user confirms
- If shifts are large, try half-edge alignment as alternative

### Step 4: Merge
- Merge all aligned, quality-approved scans
- Evaluate per-scan deviations from the merge:
  - If any scan deviates by > 3× the mean deviation, flag as outlier
  - Recommend exclusion of outliers and re-merge
- Report the merged spectrum standard deviation as a function of energy

### Step 5: Export & Report
- Generate final outputs (Athena project, CSV, plots)
- Produce a processing report including:
  - Number of scans processed / excluded
  - E0 value used
  - Normalization parameters for each scan
  - Alignment shifts
  - Quality flags and any parameter adjustments made
  - Plot of all normalized scans overlaid + merged result with error bars

## Decision Rules

- **Auto-proceed** when all quality metrics pass thresholds
- **Retry with adjusted parameters** when metrics are marginal (1 retry cycle)
- **Flag for human review** when:
  - E0 detection is ambiguous (multiple edges, weak edge step)
  - More than 30% of scans are flagged as poor quality
  - Alignment shifts are inconsistent
  - The merged standard deviation is > 5% of edge step in the XANES region
```

---

## Implementation Skeleton

```python
"""
xas_agent_tools.py — Larch-backed tool implementations for the XAS processing agent.
"""

import numpy as np
from larch import Group
from larch.io import read_ascii, read_athena, create_athena
from larch.xafs import pre_edge, autobk, align_group, merge_groups


class XASWorkspace:
    """Manages scan groups and processing state."""
    
    def __init__(self):
        self.scans: dict[str, Group] = {}
        self.processing_log: list[dict] = []
    
    def read_scan(self, filepath: str, scan_id: str = None,
                  energy_col: str = "energy", mu_col: str = "mutrans") -> dict:
        """Read a scan file into the workspace."""
        # Detect file type and read
        if filepath.endswith('.prj'):
            groups = read_athena(filepath)
            for name, grp in groups.items():
                self.scans[name] = grp
        else:
            grp = read_ascii(filepath)
            # Map columns
            grp.energy = getattr(grp, energy_col)
            grp.mu = getattr(grp, mu_col)
            sid = scan_id or filepath.stem
            self.scans[sid] = grp
        
        return self._scan_summary(sid)
    
    def normalize(self, scan_id: str, e0=None, pre1=-150, pre2=-30,
                  norm1=50, norm2=None, nnorm=2) -> dict:
        """Background subtract and normalize a scan."""
        grp = self.scans[scan_id]
        
        pre_edge(grp, e0=e0, pre1=pre1, pre2=pre2,
                 norm1=norm1, norm2=norm2, nnorm=nnorm)
        
        # Compute quality metrics
        metrics = self._normalization_quality(grp)
        
        self.processing_log.append({
            "action": "normalize",
            "scan_id": scan_id,
            "params": {"e0": grp.e0, "pre1": pre1, "pre2": pre2,
                       "norm1": norm1, "norm2": norm2, "nnorm": nnorm},
            "metrics": metrics
        })
        
        return {
            "scan_id": scan_id,
            "e0": float(grp.e0),
            "edge_step": float(grp.edge_step),
            "quality_metrics": metrics
        }
    
    def align(self, scan_ids: list[str], reference_id: str = None) -> dict:
        """Align scans to a reference."""
        ref = self.scans[reference_id or scan_ids[0]]
        shifts = {}
        
        for sid in scan_ids:
            if sid == (reference_id or scan_ids[0]):
                shifts[sid] = 0.0
                continue
            grp = self.scans[sid]
            align_group(grp, ref, array='dmude')  # align on derivative
            shifts[sid] = float(getattr(grp, 'energy_shift', 0.0))
        
        shift_vals = list(shifts.values())
        mean_shift = np.mean(shift_vals)
        std_shift = np.std(shift_vals)
        outliers = [sid for sid, s in shifts.items() 
                    if abs(s - mean_shift) > 2 * std_shift and std_shift > 0]
        
        return {
            "reference_id": reference_id or scan_ids[0],
            "shifts": shifts,
            "max_shift": float(np.max(np.abs(shift_vals))),
            "quality_metrics": {
                "mean_shift": float(mean_shift),
                "std_shift": float(std_shift),
                "outlier_scans": outliers
            }
        }
    
    def merge(self, scan_ids: list[str], exclude_ids: list[str] = None) -> dict:
        """Merge aligned, normalized scans."""
        exclude = set(exclude_ids or [])
        groups = [self.scans[sid] for sid in scan_ids if sid not in exclude]
        
        merged = merge_groups(groups, master=groups[0], xarray='energy', yarray='norm')
        self.scans["merged"] = merged
        
        # Compute per-scan deviations
        deviations = {}
        for i, sid in enumerate(sid for sid in scan_ids if sid not in exclude):
            grp = self.scans[sid]
            interp_mu = np.interp(merged.energy, grp.energy, grp.norm)
            deviations[sid] = float(np.mean(np.abs(interp_mu - merged.norm)))
        
        return {
            "merged_id": "merged",
            "n_scans_merged": len(groups),
            "excluded_scans": list(exclude),
            "quality_metrics": {
                "mean_std": float(np.mean(merged.norm_std)),
                "max_std": float(np.max(merged.norm_std)),
                "scan_deviations": deviations
            }
        }
    
    def _normalization_quality(self, grp: Group) -> dict:
        """Evaluate normalization quality metrics."""
        # Pre-edge flatness: how well the pre-edge subtraction worked
        pre_mask = grp.energy < (grp.e0 + grp.pre_edge_details.pre2)
        if np.sum(pre_mask) > 2:
            pre_resid = grp.pre_edge[pre_mask] - grp.mu[pre_mask]  
            # Simplified — real implementation would be more nuanced
            pre_flatness = 1.0 - np.std(grp.norm[pre_mask])
        else:
            pre_flatness = 0.0
        
        # Post-edge: should be ~1.0
        post_mask = grp.energy > (grp.e0 + 50)
        if np.sum(post_mask) > 2:
            post_flatness = 1.0 - np.std(grp.norm[post_mask] - 1.0)
        else:
            post_flatness = 0.0
        
        return {
            "edge_step_magnitude": float(grp.edge_step),
            "pre_edge_flatness": float(np.clip(pre_flatness, 0, 1)),
            "post_edge_flatness": float(np.clip(post_flatness, 0, 1)),
            "norm_range_eV": float(grp.energy[-1] - grp.e0)
        }


# ─── Agent orchestration via function-calling ───

TOOL_SCHEMAS = [
    {
        "name": "read_xas_scan",
        "description": "Read a raw XAS scan file into the workspace.",
        "input_schema": {
            "type": "object",
            "properties": {
                "filepath": {"type": "string"},
                "scan_id": {"type": "string"},
                "energy_col": {"type": "string", "default": "energy"},
                "mu_col": {"type": "string", "default": "mutrans"}
            },
            "required": ["filepath"]
        }
    },
    {
        "name": "pre_edge_normalize",
        "description": "Background subtract and normalize a scan. Returns quality metrics.",
        "input_schema": {
            "type": "object",
            "properties": {
                "scan_id": {"type": "string"},
                "e0": {"type": "number", "description": "Edge energy in eV. None for auto-detect."},
                "pre1": {"type": "number", "default": -150},
                "pre2": {"type": "number", "default": -30},
                "norm1": {"type": "number", "default": 50},
                "norm2": {"type": "number"},
                "nnorm": {"type": "integer", "default": 2}
            },
            "required": ["scan_id"]
        }
    },
    {
        "name": "align_scans",
        "description": "Align multiple normalized scans to a reference.",
        "input_schema": {
            "type": "object",
            "properties": {
                "scan_ids": {"type": "array", "items": {"type": "string"}},
                "reference_id": {"type": "string"}
            },
            "required": ["scan_ids"]
        }
    },
    {
        "name": "merge_scans",
        "description": "Merge aligned scans. Returns merged spectrum and deviation metrics.",
        "input_schema": {
            "type": "object",
            "properties": {
                "scan_ids": {"type": "array", "items": {"type": "string"}},
                "exclude_ids": {"type": "array", "items": {"type": "string"}, "default": []}
            },
            "required": ["scan_ids"]
        }
    },
    {
        "name": "export_results",
        "description": "Export processed data to files and generate summary plots.",
        "input_schema": {
            "type": "object",
            "properties": {
                "scan_ids": {"type": "array", "items": {"type": "string"}},
                "output_dir": {"type": "string"},
                "formats": {"type": "array", "items": {"type": "string"}, "default": ["athena_project", "csv", "plot_png"]}
            },
            "required": ["output_dir"]
        }
    }
]
```

---

## Agent Loop (Orchestrator)

```python
"""
xas_agent.py — Orchestrator that drives the LLM agent with tool-calling.
"""

import anthropic
import json

client = anthropic.Anthropic()
workspace = XASWorkspace()

SYSTEM_PROMPT = """<paste the agent system prompt from above>"""

# Map tool names to workspace methods
TOOL_DISPATCH = {
    "read_xas_scan": workspace.read_scan,
    "pre_edge_normalize": workspace.normalize,
    "align_scans": workspace.align,
    "merge_scans": workspace.merge,
    "export_results": workspace.export_results,
}


def run_agent(user_request: str, data_files: list[str]):
    """Run the XAS processing agent on a set of data files."""
    
    # Build initial message with file listing
    file_list = "\n".join(f"- {f}" for f in data_files)
    user_msg = f"{user_request}\n\nAvailable scan files:\n{file_list}"
    
    messages = [{"role": "user", "content": user_msg}]
    
    # Agent loop
    max_iterations = 50  # safety limit
    for i in range(max_iterations):
        response = client.messages.create(
            model="claude-sonnet-4-5-20250514",
            max_tokens=4096,
            system=SYSTEM_PROMPT,
            tools=TOOL_SCHEMAS,
            messages=messages
        )
        
        # Check if agent is done
        if response.stop_reason == "end_turn":
            # Extract final text response
            final_text = "".join(
                block.text for block in response.content 
                if block.type == "text"
            )
            return final_text, workspace.processing_log
        
        # Process tool calls
        if response.stop_reason == "tool_use":
            # Add assistant response to messages
            messages.append({"role": "assistant", "content": response.content})
            
            # Execute each tool call
            tool_results = []
            for block in response.content:
                if block.type == "tool_use":
                    func = TOOL_DISPATCH[block.name]
                    try:
                        result = func(**block.input)
                        tool_results.append({
                            "type": "tool_result",
                            "tool_use_id": block.id,
                            "content": json.dumps(result)
                        })
                    except Exception as e:
                        tool_results.append({
                            "type": "tool_result",
                            "tool_use_id": block.id,
                            "content": json.dumps({"error": str(e)}),
                            "is_error": True
                        })
            
            messages.append({"role": "user", "content": tool_results})
    
    return "Agent exceeded maximum iterations.", workspace.processing_log


# ─── Usage ───
if __name__ == "__main__":
    result, log = run_agent(
        user_request="Process these Fe K-edge XANES scans. Normalize, align, and merge.",
        data_files=["scan_001.dat", "scan_002.dat", "scan_003.dat", "scan_004.dat"]
    )
    print(result)
```

---

## Path Forward: Phased Development

### Phase 1: Core Tools (Week 1–2)
- Implement `XASWorkspace` with robust larch wrappers
- Handle common file formats: ASCII column data, Athena .prj, HDF5 (from specific beamlines)
- Unit test each tool with known-good reference data (e.g., Fe, S, Cu K-edge standards)
- Validate normalization quality metrics against manual Athena processing

### Phase 2: Agent Integration (Week 2–3)
- Wire up tool schemas for Claude function-calling
- Craft and iterate on the system prompt with real datasets
- Test on straightforward cases first (clean transmission XANES of a metal foil)
- Add fluorescence mode support (deadtime correction, self-absorption considerations)

### Phase 3: Quality & Edge Cases (Week 3–4)
- Test with deliberately bad data: glitchy scans, beam dumps, wrong-edge data
- Tune decision thresholds for quality gates
- Add support for beamline-specific formats and column naming conventions
- Handle multi-edge samples (e.g., Fe and S in the same dataset)

### Phase 4: User Interface & Deployment (Week 4+)
- CLI tool: `xas-agent process --files scan_*.dat --edge "Fe K"`
- Optional Jupyter notebook integration for interactive use
- Athena project export so users can verify in familiar GUI
- Processing report generation (HTML or PDF)

### Extensions
- **EXAFS processing**: Add autobk, χ(k) extraction, Fourier transform tools
- **Linear combination fitting**: Agent selects and fits reference spectra
- **Batch processing**: Process multiple samples/edges in a session
- **Beamline integration**: Watch folder for new scans, process in real-time
- **MCP server**: Wrap the workspace as an MCP server so Claude can call it natively

---

## Key Larch Functions Reference

| Task | Larch Function | Key Parameters |
|------|---------------|----------------|
| Read ASCII | `larch.io.read_ascii()` | labels, sort |
| Read Athena project | `larch.io.read_athena()` | - |
| Pre-edge & normalize | `larch.xafs.pre_edge()` | e0, pre1, pre2, norm1, norm2, nnorm |
| Find E0 | `larch.xafs.find_e0()` | - |
| Align | `larch.xafs.align_group()` | ref, array='dmude' |
| Merge | `larch.xafs.merge_groups()` | master, xarray, yarray |
| Background removal | `larch.xafs.autobk()` | rbkg, kweight |
| χ(k) → χ(R) | `larch.xafs.xftf()` | kmin, kmax, dk, window |
| Write Athena project | `larch.io.create_athena()` | - |